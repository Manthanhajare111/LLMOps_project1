import sys
import os
from operator import itemgetter
from typing import List, Optional, Dict, Any
import streamlit as st
from langchain_core.messages import BaseMessage
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_community.vectorstores import FAISS
from langchain.chains.history_aware_retriever import create_history_aware_retriever
from langchain.chains.retrieval import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from utils.model_loader import ModelLoader
from logger.custom_logger import CustomLogger
from excepctions.custom_exception import DocumentPortalException
from logger import GLOBAL_LOGGER as log
from prompt.prompt_lib import PROMPT_REGISTRY
from model.models import PromptType

from langchain_core.runnables.history import RunnableWithMessageHistory

class ConversationalRAG:
   def __init__(self,session_id:str, retriever=None):
    try:
        self.log=CustomLogger().get_logger(__file__)
        self.session_id = session_id
        self.retriever=retriever
        self.llm= self._load_llm()
        self.contextualize_prompt = PROMPT_REGISTRY[PromptType.CONTEXUALIZE_QUESTION.value]
        self.qa_prompt = PROMPT_REGISTRY[PromptType.CONTEXT_QA.value]
        if retriever is None:
            raise ValueError("Retriever must be provided")
        self.retriever=retriever
        self._build_lcel_chain()
        self.log.info("ConversationalRAG initialized",session_id=session_id)

    except Exception as e:
        log.error("Failed to initialize ConversationalRAG",error=str(e))    
        raise DocumentPortalException("ConversationalRAG Initialization Failed", sys)
    
    def load_retriever_from_faisse(self,index_path:str):
        """Load retriever from FAISS index."""
        try:
            embeddings = ModelLoader.load_embeddings()
            if not os.path.exists(index_path):
                raise FileNotFoundError(f"FAISS index not found at {index_path}")
            
            vectorstore = FAISS.load_local(folder_path=index_path, embeddings=embeddings,allow_dangerous_deserialization=True)
            self.log.info("FAISS index loaded successfully",index_path=index_path)
            self._build_lcel_chain()

            return self.retriever
        
        except Exception as e:
            log.error("Failed to load retriever",error=str(e))
            raise DocumentPortalException("Retriever Loading Failed", sys)

    def invoke(self,user_input:str,chat_history: Optional[List[BaseMessage]] = None) -> str:
        """
        Args:
            user_input (str): The user's input question.
            chat_history (Optional[List[BaseMessage]]): The chat history as a list of BaseMessage objects."""
        try:
            payload = {"input": user_input, "chat_history": chat_history or []}
            response = self.chain.invoke(payload)
            if response is None:
                self.log.warning("No response generated by the chain",session_id=self.session_id)
                return "I'm sorry, I couldn't generate a response at this time."
            self.log.info("Response generated successfully",
                          session_id=self.session_id,
                          user_input=user_input,
                          answer_preview=response[:150]
                          )
            return response
        except Exception as e:  
            log.error("Failed to invoke ConversationalRAG",error=str(e))    
            raise DocumentPortalException("ConversationalRAG Invocation Failed", sys)
        
    def _load_llm(self):
        try:
            llm = ModelLoader.load_llm()
            self.log.info("LLM loaded successfully",class_name= llm.__class__.__name__)
            return llm
        except Exception as e:
            log.error("Failed to load LLM",error=str(e))
            raise DocumentPortalException("LLM Loading Failed", sys)
        
    @staticmethod
    def _format_docs(docs):
        return "\n\n.join(doc.page_content for doc in docs)"
        
    def _build_lcel_chain(self):
        try:
            question_rewriter = (
                {"input":itemgetter("input"),"chat_history":itemgetter("chat_history")}
                | self.contextualize_prompt
                | self.llm
                | StrOutputParser()
            )
            retriever_docs = question_rewriter | self.retriever | self._format_docs

            self.chain = (
                {"context":retriever_docs,"input":itemgetter("input"),"chat_history":itemgetter("chat_history")}
                | self.qa_prompt
                | self.llm
                | StrOutputParser()
            )
        except Exception as e:
            log.error("Failed to build LLM chain",error=str(e))
            raise DocumentPortalException("LLM Chain Building Failed", sys)